{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37997396",
   "metadata": {},
   "source": [
    "# Estimated time: 15 min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f990e6e0",
   "metadata": {},
   "source": [
    "In this tutorial, we will explore how to manage Sktime models in MLflow using the [MLflavors library](https://github.com/ml-toolkits/mlflavors).\n",
    "\n",
    "By following this guide, you'll learn how to:\n",
    "\n",
    "- Save Sktime models as MLflow artifacts.\n",
    "- Load Sktime models from MLflow for batch inference.\n",
    "- Deploy Sktime models to a serving endpoint using MLflow deployment tools.\n",
    "\n",
    "Note: The Sktime custom flavor interface design in the MLflavors library follows the Sktime example in the [MLflow documentaion](https://mlflow.org/docs/latest/models.html#custom-flavors). Particularly, the interface for utilizing the model loaded as a `pyfunc` flavor for generating predictions uses a single-row Pandas DataFrame configuration argument to expose the\n",
    "parameters of the Sktime inference API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae526e0a",
   "metadata": {},
   "source": [
    "## Saving the model as an MLflow artifact\n",
    "This tutorial trains a Sktime NaiveForecaster model using the Longley dataset for forecasting with exogenous variables. A new MLflow experiment is created to log the model parameters, evaluation metrics and the trained model as an artifact and forecasts are computed loading the trained model in native flavor and pyfunc flavor. Finally, the model is served for real-time inference to a local endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25fb2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import mlflavors\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from sktime.datasets import load_longley\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sktime.performance_metrics.forecasting import (\n",
    "    mean_absolute_error,\n",
    "    mean_absolute_percentage_error,\n",
    ")\n",
    "\n",
    "\n",
    "ARTIFACT_PATH = \"model\"\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    y, X = load_longley()\n",
    "    y_train, y_test, X_train, X_test = temporal_train_test_split(y, X)\n",
    "\n",
    "    forecaster = NaiveForecaster()\n",
    "    forecaster.fit(\n",
    "        y_train,\n",
    "        X=X_train,\n",
    "        fh=[1, 2, 3, 4],\n",
    "    )\n",
    "\n",
    "    # Extract parameters\n",
    "    parameters = forecaster.get_params()\n",
    "\n",
    "    # Evaluate model\n",
    "    y_pred = forecaster.predict(X=X_test)\n",
    "    metrics = {\n",
    "        \"mae\": mean_absolute_error(y_test, y_pred),\n",
    "        \"mape\": mean_absolute_percentage_error(y_test, y_pred),\n",
    "    }\n",
    "\n",
    "    print(f\"Parameters: \\n{json.dumps(parameters, indent=2)}\")\n",
    "    print(f\"\\nMetrics: \\n{json.dumps(metrics, indent=2)}\")\n",
    "\n",
    "    # Log parameters and metrics\n",
    "    mlflow.log_params(parameters)\n",
    "    mlflow.log_metrics(metrics)\n",
    "\n",
    "    # Log model to MLflow tracking server\n",
    "    mlflavors.sktime.log_model(\n",
    "        sktime_model=forecaster,\n",
    "        artifact_path=ARTIFACT_PATH,\n",
    "    )\n",
    "    \n",
    "    # Return model uri from the current run\n",
    "    model_uri = mlflow.get_artifact_uri(ARTIFACT_PATH)\n",
    "    \n",
    "# Print the run id wich is used below for serving the model to a local REST API endpoint\n",
    "print(f\"\\nMLflow run id:\\n{run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df09b27",
   "metadata": {},
   "source": [
    "## Viewing the model in the MLflow UI\n",
    "To view the run output in the MLflow UI run the following command:\n",
    "\n",
    "```bash\n",
    "mlflow ui\n",
    "```\n",
    "\n",
    "When opening the MLflow runs detail page the serialized model artifact will show up, such as:\n",
    "\n",
    "![title](../images/tracking_artifact_ui.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f2f7c6",
   "metadata": {},
   "source": [
    "## Loading the model from MLflow\n",
    "Make a prediction loading the model from MLflow in native format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b310fcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = mlflavors.sktime.load_model(model_uri=model_uri)\n",
    "print(loaded_model.predict_interval(fh=[1, 2, 3], X=X_test, coverage=[0.9, 0.95]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5ec99a",
   "metadata": {},
   "source": [
    "Make a prediction loading the model from MLflow in `pyfunc` format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea02a4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test data to numpy array so it can be passed to pyfunc predict using\n",
    "# a single-row Pandas DataFrame configuration argument\n",
    "X_test_array = X_test.to_numpy()\n",
    "\n",
    "# Create configuration DataFrame for interval forecast with nominal coverage\n",
    "# value [0.9,0.95], future forecast horizon of 3 periods, and exogenous regressor.\n",
    "predict_conf = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"fh\": [1, 2, 3],\n",
    "            \"predict_method\": \"predict_interval\",\n",
    "            \"coverage\": [0.9, 0.95],\n",
    "            \"X\": X_test_array,\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "loaded_pyfunc = mlflavors.sktime.pyfunc.load_model(model_uri=model_uri)\n",
    "print(loaded_pyfunc.predict(predict_conf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07ccb6a",
   "metadata": {},
   "source": [
    "# Serving the model to an endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3949845",
   "metadata": {},
   "source": [
    "To serve the model to a local REST API endpoint run the command below where you substitute the run id printed above (similarly, you could serve the model to an endpoint in the cloud (e.g. Azure ML, AWS SageMaker, etc.) using [MLflow deployment tools](https://mlflow.org/docs/latest/models.html#built-in-deployment-tools)):\n",
    "\n",
    "```bash\n",
    "mlflow models serve -m runs:/<run_id>/model --env-manager local --host 127.0.0.1\n",
    "```\n",
    "\n",
    "Open a new terminal and run the below model scoring script to request a prediction from the served model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c11c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from sktime.datasets import load_longley\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "\n",
    "y, X = load_longley()\n",
    "y_train, y_test, X_train, X_test = temporal_train_test_split(y, X)\n",
    "\n",
    "# Define local host and endpoint url\n",
    "host = \"127.0.0.1\"\n",
    "url = f\"http://{host}:5000/invocations\"\n",
    "\n",
    "# Model scoring via REST API requires transforming the configuration DataFrame\n",
    "# into JSON format. As numpy ndarray type is not JSON serializable we need to\n",
    "# convert the exogenous regressor into a list. The wrapper instance will convert\n",
    "# the list back to ndarray type as required by sktime predict methods. For more\n",
    "# details read the MLflow deployment API reference.\n",
    "# (https://mlflow.org/docs/latest/models.html#deploy-mlflow-models)\n",
    "X_test_list = X_test.to_numpy().tolist()\n",
    "predict_conf = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"fh\": [1, 2, 3],\n",
    "            \"predict_method\": \"predict_interval\",\n",
    "            \"coverage\": [0.9, 0.95],\n",
    "            \"X\": X_test_list,\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create dictionary with pandas DataFrame in the split orientation\n",
    "json_data = {\"dataframe_split\": predict_conf.to_dict(orient=\"split\")}\n",
    "\n",
    "# Score model\n",
    "response = requests.post(url, json=json_data)\n",
    "print(response.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
